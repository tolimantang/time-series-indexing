apiVersion: batch/v1
kind: Job
metadata:
  name: oil-futures-backfill
  namespace: time-series-indexing
  labels:
    app: oil-futures-backfill
    component: data-backfill
    type: historical-data
spec:
  # Allow manual cleanup, job will not be auto-deleted
  ttlSecondsAfterFinished: 86400  # Keep for 24 hours after completion
  template:
    metadata:
      labels:
        app: oil-futures-backfill
        component: data-backfill
    spec:
      containers:
      - name: oil-futures-backfill
        image: python:3.11-slim
        command:
        - /bin/bash
        - -c
        - |
          set -e
          echo "üöÄ Starting Oil Futures Backfill Job"
          echo "Timestamp: $(date -u +"%Y-%m-%dT%H:%M:%SZ")"

          # Install system dependencies
          apt-get update && apt-get install -y \
            libpq-dev \
            gcc \
            curl \
            && rm -rf /var/lib/apt/lists/*

          # Install Python dependencies
          pip install --no-cache-dir \
            psycopg2-binary \
            requests \
            pandas \
            pyyaml \
            numpy

          # Create temporary directory for the script
          mkdir -p /tmp/backfill
          cd /tmp/backfill

          # Download the backfill script (replace with your actual source)
          cat > oil_futures_backfill.py << 'EOFSCRIPT'
          #!/usr/bin/env python3
          """
          Oil Futures Historical Data Backfill Script for Kubernetes
          Fetches 30 years of oil futures daily data and stores in PostgreSQL.
          """

          import os
          import sys
          import logging
          import argparse
          import psycopg2
          import pandas as pd
          import requests
          from datetime import datetime, timedelta
          from typing import Dict, Any

          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
              handlers=[logging.StreamHandler(sys.stdout)]
          )
          logger = logging.getLogger(__name__)


          class PostgresOnlyEncoder:
              """Minimal PostgreSQL encoder for oil futures data."""

              def __init__(self, db_config: Dict[str, str]):
                  self.db_config = db_config
                  self._test_connection()

              def _test_connection(self):
                  """Test database connection."""
                  try:
                      conn = psycopg2.connect(**self.db_config)
                      cursor = conn.cursor()
                      cursor.execute("SELECT 1")
                      cursor.close()
                      conn.close()
                      logger.info("‚úÖ PostgreSQL connection successful")
                  except Exception as e:
                      logger.error(f"‚ùå PostgreSQL connection failed: {e}")
                      raise

              def store_market_data_postgres(self, symbol: str, data: pd.DataFrame):
                  """Store market data in PostgreSQL."""
                  if data.empty:
                      logger.warning(f"No data to store for {symbol}")
                      return

                  logger.info(f"üìä Storing {len(data)} records for {symbol} to PostgreSQL...")

                  conn = None
                  cursor = None
                  try:
                      conn = psycopg2.connect(**self.db_config)
                      cursor = conn.cursor()

                      # Prepare records
                      records = []
                      for date, row in data.iterrows():
                          trade_date = date.date() if hasattr(date, 'date') else date
                          record = (
                              str(symbol),
                              trade_date,
                              float(row.get('open', row['close'])),
                              float(row.get('high', row['close'])),
                              float(row.get('low', row['close'])),
                              float(row['close']),
                              float(row.get('adjusted_close', row['close'])),
                              int(row.get('volume', 0)),
                              float(row.get('daily_return', 0)) if pd.notna(row.get('daily_return')) else None
                          )
                          records.append(record)

                      # Insert with conflict handling
                      insert_query = """
                          INSERT INTO market_data (
                              symbol, trade_date, open_price, high_price, low_price,
                              close_price, adjusted_close, volume, daily_return
                          ) VALUES %s
                          ON CONFLICT (symbol, trade_date)
                          DO UPDATE SET
                              open_price = EXCLUDED.open_price,
                              high_price = EXCLUDED.high_price,
                              low_price = EXCLUDED.low_price,
                              close_price = EXCLUDED.close_price,
                              adjusted_close = EXCLUDED.adjusted_close,
                              volume = EXCLUDED.volume,
                              daily_return = EXCLUDED.daily_return,
                              updated_at = NOW()
                      """

                      from psycopg2.extras import execute_values
                      execute_values(cursor, insert_query, records)
                      conn.commit()
                      logger.info(f"‚úÖ Successfully stored {len(records)} records for {symbol}")

                  except Exception as e:
                      logger.error(f"‚ùå Error storing data for {symbol}: {e}")
                      if conn:
                          conn.rollback()
                      raise
                  finally:
                      if cursor:
                          cursor.close()
                      if conn:
                          conn.close()


          class YahooFinanceSource:
              """Yahoo Finance data source."""

              def get_daily_data(self, symbol: str) -> pd.DataFrame:
                  """Get daily data from Yahoo Finance."""
                  try:
                      # Get 30+ years of data
                      end_time = int(datetime.now().timestamp())
                      start_time = int((datetime.now() - timedelta(days=30*365)).timestamp())

                      url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
                      params = {
                          'period1': start_time,
                          'period2': end_time,
                          'interval': '1d'
                      }
                      headers = {
                          'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                      }

                      response = requests.get(url, params=params, headers=headers, timeout=30)
                      response.raise_for_status()
                      data = response.json()

                      if 'chart' not in data or not data['chart']['result']:
                          logger.error(f"No data found for {symbol}")
                          return pd.DataFrame()

                      result = data['chart']['result'][0]
                      timestamps = result['timestamp']
                      quotes = result['indicators']['quote'][0]

                      df = pd.DataFrame({
                          'open': quotes['open'],
                          'high': quotes['high'],
                          'low': quotes['low'],
                          'close': quotes['close'],
                          'volume': quotes['volume']
                      })

                      df.index = pd.to_datetime([datetime.fromtimestamp(ts) for ts in timestamps])
                      df = df.dropna().sort_index()

                      logger.info(f"Fetched {len(df)} days of data for {symbol}")
                      return df

                  except Exception as e:
                      logger.error(f"Error fetching data for {symbol}: {e}")
                      return pd.DataFrame()


          def main():
              """Main backfill function."""
              parser = argparse.ArgumentParser(description='Oil Futures Backfill')
              parser.add_argument('--symbols', nargs='+', choices=['wti', 'brent', 'all'], default=['all'])
              parser.add_argument('--batch-size', type=int, default=500)

              args = parser.parse_args()

              # Database configuration from environment
              db_config = {
                  'host': os.environ['DB_HOST'],
                  'port': os.environ.get('DB_PORT', '5432'),
                  'database': os.environ['DB_NAME'],
                  'user': os.environ['DB_USER'],
                  'password': os.environ['DB_PASSWORD'],
              }

              # Oil futures configuration
              oil_config = {
                  'wti': {'yahoo_symbol': 'CL=F', 'db_symbol': 'CRUDE_OIL_WTI', 'name': 'WTI Crude Oil'},
                  'brent': {'yahoo_symbol': 'BZ=F', 'db_symbol': 'CRUDE_OIL_BRENT', 'name': 'Brent Crude Oil'}
              }

              # Determine symbols to process
              if 'all' in args.symbols:
                  symbols_to_process = ['wti', 'brent']
              else:
                  symbols_to_process = args.symbols

              logger.info(f"üõ¢Ô∏è  Processing oil futures: {symbols_to_process}")

              # Initialize components
              encoder = PostgresOnlyEncoder(db_config)
              yahoo = YahooFinanceSource()

              success_count = 0

              for symbol_key in symbols_to_process:
                  config = oil_config[symbol_key]
                  logger.info(f"üìà Processing {config['name']} ({config['yahoo_symbol']})...")

                  # Fetch data
                  data = yahoo.get_daily_data(config['yahoo_symbol'])
                  if data.empty:
                      logger.error(f"‚ùå No data for {config['name']}")
                      continue

                  # Calculate returns
                  data['daily_return'] = data['close'].pct_change()

                  # Process in batches
                  total_records = len(data)
                  for i in range(0, total_records, args.batch_size):
                      batch_data = data.iloc[i:i+args.batch_size].copy()
                      try:
                          encoder.store_market_data_postgres(config['db_symbol'], batch_data)
                      except Exception as e:
                          logger.error(f"‚ùå Batch failed: {e}")
                          continue

                  success_count += 1
                  logger.info(f"‚úÖ {config['name']} completed: {len(data)} records")

              logger.info(f"üéâ Backfill completed: {success_count}/{len(symbols_to_process)} symbols processed")
              return 0 if success_count == len(symbols_to_process) else 1


          if __name__ == "__main__":
              sys.exit(main())
          EOFSCRIPT

          # Make script executable and run it
          chmod +x oil_futures_backfill.py

          echo "üìä Starting oil futures data backfill..."
          python3 oil_futures_backfill.py --symbols all --batch-size 500

          echo "üéâ Oil futures backfill job completed!"

        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: market-encoder-secrets
              key: db-host
        - name: DB_PORT
          value: "5432"
        - name: DB_NAME
          valueFrom:
            secretKeyRef:
              name: market-encoder-secrets
              key: db-name
        - name: DB_USER
          valueFrom:
            secretKeyRef:
              name: market-encoder-secrets
              key: db-user
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: market-encoder-secrets
              key: db-password

        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"

        # Add volume for temporary storage if needed
        volumeMounts:
        - name: tmp-storage
          mountPath: /tmp

      volumes:
      - name: tmp-storage
        emptyDir:
          sizeLimit: 1Gi

      restartPolicy: Never

      # Tolerate CriticalAddonsOnly taint
      tolerations:
      - key: "CriticalAddonsOnly"
        operator: "Exists"
        effect: "NoSchedule"
      - key: "CriticalAddonsOnly"
        operator: "Exists"
        effect: "NoExecute"

      # Optional: Add node selector for specific node types
      # nodeSelector:
      #   workload-type: data-processing

  # Retry failed jobs up to 3 times
  backoffLimit: 3

  # Optional: Set active deadline (max run time) to 2 hours
  activeDeadlineSeconds: 7200